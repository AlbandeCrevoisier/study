### A Pluto.jl notebook ###
# v0.12.21

using Markdown
using InteractiveUtils

# This Pluto notebook uses @bind for interactivity. When running this notebook outside of Pluto, the following 'mock version' of @bind gives bound variables a default value (instead of an error).
macro bind(def, element)
    quote
        local el = $(esc(element))
        global $(esc(def)) = Core.applicable(Base.get, el) ? Base.get(el) : missing
        el
    end
end

# ‚ïî‚ïê‚ï° c6c4fdc2-82ad-11eb-044a-abace30e2eac
using Distributions, Plots, PlutoUI

# ‚ïî‚ïê‚ï° d0469884-7eba-11eb-1d19-eb3680e350a3
md"
# 10-armed Testbed

Non-associative k-armed bandit is the classical reinforcement learning toy problem.

Bandits are casino machines where one pulls a lever to obtain a reward, each iteration being independant from the previous ones. In the k-armed version, there are k-levers, each being associated with a reward function. The goal is to maximise the gains over a set period of time: this involves balancing exploration to estimate the expectation of each reward distribution as well as exploitation to maximise the gains ‚Äî playing the lever associated with the highest estimated value.

A possible solution is found in œµ-greedy methods: start with a set of prior value estimates, then at each step, pick a random action (lever) with probability œµ, otherwise pick the greedy action (the highest estimated value).
* œµ may be constant (useful to track changes in the reward function) or decreasing over time.
* Various strategies are possible to estimate values: often, one uses sample-average.

This testbed compares various values of œµ on 2,000 10-armed bandits for 1,000 steps.

##### Notations

ùíú: set of actions\
q: state value\
r: reward\

##### 10-armed Bandit

‚àÄ a ‚àà ùíú, q\*(a): true value, sampled from ùí©(0, 1).\
Rewards: sampled from ùí©(q\*(a), 1).


##### œµ-greedy method

Initialisation: ‚àÄ a ‚àà ùíú, q(a) = 0.\
p(œµ): take a random action.\
p(1 - œµ): take the greedy action.\
Update the action-value estimate with a sample-average.\
"

# ‚ïî‚ïê‚ï° a317775a-8826-11eb-2c46-bfc7cdf28001
function makebandit(k=10)
	q‚Çì = rand(Normal(), k)
	a -> rand(Normal(q‚Çì[a]))
end

# ‚ïî‚ïê‚ï° 55683178-82b0-11eb-21a0-118a2c5910ed
function makeœµgreedy(œµ, k=10)
	# First row: sample-average.
	# Second row: count.
	sample_average = zeros(2, k)
	# Pick an action
	function f()
		# Explore
		if rand(Uniform()) < œµ
			rand(1:k)
		else
			# Exploit
			argmax(sample_average[1, :])
		end
	end
	# Update sample-average value estimate.
	function f(a, r)
		prev, count = sample_average[:, a]
		sample_average[1, a] = (prev*count + r) / (count + 1)
		sample_average[2, a] = count + 1
	end
end

# ‚ïî‚ïê‚ï° b9d1fd58-8830-11eb-2958-d93a88a14079
function playnsteps(bandit, œµgreedy, n=1000)
	rewards = zeros(n)
	for i in 1:n
		a = œµgreedy()
		r = bandit(a)
		œµgreedy(a, r)
		rewards[i] = r
	end
	rewards
end

# ‚ïî‚ïê‚ï° 55eb9ab4-8831-11eb-3e19-af7557572afd
@bind œµ Slider(0:0.01:0.1; show_value=true)

# ‚ïî‚ïê‚ï° 2af0f768-882f-11eb-11ca-39645ca19ff5
rewards = playnsteps(makebandit(), makeœµgreedy(œµ))

# ‚ïî‚ïê‚ï° 46ed3088-882f-11eb-1e36-b1766d7cf523
plot(1:1000, rewards)

# ‚ïî‚ïê‚ï° Cell order:
# ‚ïü‚îÄd0469884-7eba-11eb-1d19-eb3680e350a3
# ‚ïü‚îÄc6c4fdc2-82ad-11eb-044a-abace30e2eac
# ‚ïü‚îÄa317775a-8826-11eb-2c46-bfc7cdf28001
# ‚ïü‚îÄ55683178-82b0-11eb-21a0-118a2c5910ed
# ‚ïü‚îÄb9d1fd58-8830-11eb-2958-d93a88a14079
# ‚ïü‚îÄ55eb9ab4-8831-11eb-3e19-af7557572afd
# ‚ïü‚îÄ2af0f768-882f-11eb-11ca-39645ca19ff5
# ‚ïü‚îÄ46ed3088-882f-11eb-1e36-b1766d7cf523
