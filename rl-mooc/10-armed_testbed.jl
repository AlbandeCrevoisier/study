### A Pluto.jl notebook ###
# v0.12.21

using Markdown
using InteractiveUtils

# â•”â•â•¡ c6c4fdc2-82ad-11eb-044a-abace30e2eac
using Distributions, Plots, PlutoUI, Statistics

# â•”â•â•¡ d0469884-7eba-11eb-1d19-eb3680e350a3
md"
# 10-armed Testbed

Non-associative k-armed bandit is the classical reinforcement learning toy problem.

Bandits are casino machines where one pulls a lever to obtain a reward, each iteration being independant from the previous ones. In the k-armed version, there are k-levers, each being associated with a reward function. The goal is to maximise the gains over a set period of time: this involves balancing exploration to estimate the expectation of each reward distribution as well as exploitation to maximise the gains â€” playing the lever associated with the highest estimated value.

A possible solution is found in Ïµ-greedy methods: start with a set of prior value estimates, then at each step, pick a random action (lever) with probability Ïµ, otherwise pick the greedy action (the highest estimated value).
* Ïµ may be constant (useful to track changes in the reward function) or decreasing over time.
* Various strategies are possible to estimate values: often, one uses sample-average.

This testbed compares various values of Ïµ on 2,000 10-armed bandits for 1,000 steps.

##### Notations

ğ’œ: set of actions\
q: state value\
r: reward\

##### 10-armed Bandit

âˆ€ a âˆˆ ğ’œ, q\*(a): true value, sampled from ğ’©(0, 1).\
Rewards: sampled from ğ’©(q\*(a), 1).


##### Ïµ-greedy method

Initialisation: âˆ€ a âˆˆ ğ’œ, q(a) = 0.\
p(Ïµ): take a random action.\
p(1 - Ïµ): take the greedy action.\
Update the action-value estimate with a sample-average.\
"

# â•”â•â•¡ a317775a-8826-11eb-2c46-bfc7cdf28001
function makebandit(k=10)
	qâ‚“ = rand(Normal(), k)
	a -> rand(Normal(qâ‚“[a]))
end

# â•”â•â•¡ 55683178-82b0-11eb-21a0-118a2c5910ed
function makeÏµgreedy(Ïµ, k=10)
	# First row: sample-average.
	# Second row: count.
	sample_average = zeros(2, k)
	# Pick an action
	function f()
		# Explore
		if rand(Uniform()) < Ïµ
			rand(1:k)
		else
			# Exploit
			argmax(sample_average[1, :])
		end
	end
	# Update sample-average value estimate.
	function f(a, r)
		prev, count = sample_average[:, a]
		sample_average[1, a] = (prev*count + r) / (count + 1)
		sample_average[2, a] = count + 1
	end
end

# â•”â•â•¡ b9d1fd58-8830-11eb-2958-d93a88a14079
function playnsteps(bandit, Ïµgreedy, n=1000)
	rewards = Array{Float64}(undef, n)
	for i in 1:n
		a = Ïµgreedy()
		r = bandit(a)
		Ïµgreedy(a, r)
		rewards[i] = r
	end
	rewards
end

# â•”â•â•¡ 2af0f768-882f-11eb-11ca-39645ca19ff5
rewards = [mean([playnsteps(makebandit(), makeÏµgreedy(Ïµ)) for _ in 1:2000]) for Ïµ in [0, 0.01, 0.1, 0.5]]

# â•”â•â•¡ 1db37098-8a8b-11eb-0e02-993f6e332c7b
pgfplotsx()

# â•”â•â•¡ 46ed3088-882f-11eb-1e36-b1766d7cf523
plot(1:1000, rewards, label = ["0" "0.01" "0.1" "0.5"])

# â•”â•â•¡ adc5fd18-8d7e-11eb-2367-1906b97d68fc
md"### Nonstationary bandits"

# â•”â•â•¡ 1849dc46-8d7e-11eb-2e96-ebbd410da0b9
function makenonstationarybandit(k=10)
	qâ‚“ = zeros(k)
	a -> begin
		qâ‚“ += rand(Normal(0, 0.01), k)
		rand(Normal(qâ‚“[a]))
	end
end

# â•”â•â•¡ d066ae2e-8d7e-11eb-3c95-6b5ac74cd2af
nsrewards = [mean([playnsteps(makenonstationarybandit(), makeÏµgreedy(Ïµ), 10000) for _ in 1:2000]) for Ïµ in [0, 0.01, 0.1, 0.5]]

# â•”â•â•¡ f91cb75c-8d7e-11eb-23d4-83799d5f050f
plot(1:10000, nsrewards, label = ["0" "0.01" "0.1" "0.5"])

# â•”â•â•¡ Cell order:
# â•Ÿâ”€d0469884-7eba-11eb-1d19-eb3680e350a3
# â•Ÿâ”€c6c4fdc2-82ad-11eb-044a-abace30e2eac
# â•Ÿâ”€a317775a-8826-11eb-2c46-bfc7cdf28001
# â•Ÿâ”€55683178-82b0-11eb-21a0-118a2c5910ed
# â•Ÿâ”€b9d1fd58-8830-11eb-2958-d93a88a14079
# â•Ÿâ”€2af0f768-882f-11eb-11ca-39645ca19ff5
# â•Ÿâ”€1db37098-8a8b-11eb-0e02-993f6e332c7b
# â• â•46ed3088-882f-11eb-1e36-b1766d7cf523
# â•Ÿâ”€adc5fd18-8d7e-11eb-2367-1906b97d68fc
# â•Ÿâ”€1849dc46-8d7e-11eb-2e96-ebbd410da0b9
# â•Ÿâ”€d066ae2e-8d7e-11eb-3c95-6b5ac74cd2af
# â•Ÿâ”€f91cb75c-8d7e-11eb-23d4-83799d5f050f
