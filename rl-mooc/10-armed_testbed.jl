### A Pluto.jl notebook ###
# v0.12.21

using Markdown
using InteractiveUtils

# ‚ïî‚ïê‚ï° c6c4fdc2-82ad-11eb-044a-abace30e2eac
using Distributions, Plots, PlutoUI, Statistics

# ‚ïî‚ïê‚ï° d0469884-7eba-11eb-1d19-eb3680e350a3
md"
# 10-armed Testbed

Non-associative k-armed bandit is the classical reinforcement learning toy problem.

Bandits are casino machines where one pulls a lever to obtain a reward, each iteration being independant from the previous ones. In the k-armed version, there are k-levers, each being associated with a reward function. The goal is to maximise the gains over a set period of time: this involves balancing exploration to estimate the expectation of each reward distribution as well as exploitation to maximise the gains ‚Äî playing the lever associated with the highest estimated value.

A possible solution is found in œµ-greedy methods: start with a set of prior value estimates, then at each step, pick a random action (lever) with probability œµ, otherwise pick the greedy action (the highest estimated value).
* œµ may be constant (useful to track changes in the reward function) or decreasing over time.
* Various strategies are possible to estimate values: often, one uses sample-average.

This testbed compares various values of œµ on 2,000 10-armed bandits for 1,000 steps.

##### Notations

ùíú: set of actions\
q: state value\
r: reward\

##### 10-armed Bandit

‚àÄ a ‚àà ùíú, q\*(a): true value, sampled from ùí©(0, 1).\
Rewards: sampled from ùí©(q\*(a), 1).


##### œµ-greedy method

Initialisation: ‚àÄ a ‚àà ùíú, q(a) = 0.\
p(œµ): take a random action.\
p(1 - œµ): take the greedy action.\
Update the action-value estimate with a sample-average.\
"

# ‚ïî‚ïê‚ï° a317775a-8826-11eb-2c46-bfc7cdf28001
function makebandit(k=10)
	q‚Çì = rand(Normal(), k)
	a -> rand(Normal(q‚Çì[a]))
end

# ‚ïî‚ïê‚ï° 55683178-82b0-11eb-21a0-118a2c5910ed
function makeœµgreedy(œµ, k=10)
	# First row: sample-average.
	# Second row: count.
	sample_average = zeros(2, k)
	# Pick an action
	function f()		
		if rand(Uniform()) < œµ
			# Explore
			rand(1:k)
		else
			# Exploit
			argmax(sample_average[1, :])
		end
	end
	# Update sample-average value estimate.
	function f(a, r)
		prev, count = sample_average[:, a]
		sample_average[1, a] = (prev*count + r) / (count + 1)
		sample_average[2, a] = count + 1
	end
end

# ‚ïî‚ïê‚ï° b9d1fd58-8830-11eb-2958-d93a88a14079
function playnsteps(bandit, œµgreedy, n=1000)
	rewards = Array{Float64}(undef, n)
	for i in 1:n
		a = œµgreedy()
		r = bandit(a)
		œµgreedy(a, r)
		rewards[i] = r
	end
	rewards
end

# ‚ïî‚ïê‚ï° 2af0f768-882f-11eb-11ca-39645ca19ff5
rewards = [mean([playnsteps(makebandit(), makeœµgreedy(œµ)) for _ in 1:2000]) for œµ in [0, 0.01, 0.1, 0.5]]

# ‚ïî‚ïê‚ï° 1db37098-8a8b-11eb-0e02-993f6e332c7b
pgfplotsx()

# ‚ïî‚ïê‚ï° 46ed3088-882f-11eb-1e36-b1766d7cf523
plot(1:1000, rewards, label = ["0" "0.01" "0.1" "0.5"])

# ‚ïî‚ïê‚ï° adc5fd18-8d7e-11eb-2367-1906b97d68fc
md"### Nonstationary bandits"

# ‚ïî‚ïê‚ï° 1849dc46-8d7e-11eb-2e96-ebbd410da0b9
function makenonstationarybandit(k=10)
	q‚Çì = zeros(k)
	a -> begin
		q‚Çì += rand(Normal(0, 0.01), k)
		rand(Normal(q‚Çì[a]))
	end
end

# ‚ïî‚ïê‚ï° d066ae2e-8d7e-11eb-3c95-6b5ac74cd2af
nsrewards = [mean([playnsteps(makenonstationarybandit(), makeœµgreedy(œµ), 10000) for _ in 1:2000]) for œµ in [0, 0.01, 0.1, 0.5]]

# ‚ïî‚ïê‚ï° f91cb75c-8d7e-11eb-23d4-83799d5f050f
plot(1:10000, nsrewards, label = ["0" "0.01" "0.1" "0.5"])

# ‚ïî‚ïê‚ï° f988df5e-8f36-11eb-2d8e-954e6e7986e6
md"### Optimistic prior"

# ‚ïî‚ïê‚ï° 0632c6a2-8f37-11eb-09b7-237be141ef01
function makeoptimisticœµgreedy(œµ, k=10)
	# First row: sample-average.
	# Second row: count.
	sample_average = zeros(2, k)
	# Optimistic prior
	sample_average[1, :] .= 5
	# Pick an action
	function f()
		if rand(Uniform()) < œµ
			# Explore
			rand(1:k)
		else
			# Exploit
			argmax(sample_average[1, :])
		end
	end
	# Update sample-average value estimate.
	function f(a, r)
		prev, count = sample_average[:, a]
		sample_average[1, a] = (prev*count + r) / (count + 1)
		sample_average[2, a] = count + 1
	end
end

# ‚ïî‚ïê‚ï° 61eaa072-8f38-11eb-3f3e-8d5160d91d3a
optimisticrewards = [mean([playnsteps(makebandit(), makeoptimisticœµgreedy(œµ)) for _ in 1:2000]) for œµ in [0, 0.01, 0.1, 0.5]]

# ‚ïî‚ïê‚ï° 75da65ae-8f38-11eb-1743-35205166384d
plot(1:1000, optimisticrewards, label = ["0" "0.01" "0.1" "0.5"])

# ‚ïî‚ïê‚ï° 37d09eb8-8f3a-11eb-08eb-2713d85bd6f6
md"### Upper Confidence Bound"

# ‚ïî‚ïê‚ï° 4308cefc-8f3a-11eb-25e6-774eac28ce02
function makeucb(c=2)
	q = 5 .* ones(10)
	counts = zeros(10)
	# Pick an action
	function f()
		# in Julia, x / 0 = +‚àû
		if counts == zeros(10)
			1
		else
			argmax([q[i] + c * sqrt(log(sum(counts))/counts[i]) for i in 1:10])
		end
	end
	# Update action values with upper confidence bound
	function f(a ,r)
		counts[a] += 1
		q[a] = q[a] + (r - q[a]) / counts[a]
	end
end

# ‚ïî‚ïê‚ï° e0e73616-8f3c-11eb-01fd-530ab31b3270
ucbrewards = mean([playnsteps(makebandit(), makeucb()) for _ in 1:2000])

# ‚ïî‚ïê‚ï° 220f8b34-8f3d-11eb-1b49-6be6a6faa650
begin
	plot(1:1000, ucbrewards, label = "c = 2")
	plot!(1:1000, rewards[2], label = "œµ = 0. 01")
end

# ‚ïî‚ïê‚ï° Cell order:
# ‚ïü‚îÄd0469884-7eba-11eb-1d19-eb3680e350a3
# ‚ïü‚îÄc6c4fdc2-82ad-11eb-044a-abace30e2eac
# ‚ïü‚îÄa317775a-8826-11eb-2c46-bfc7cdf28001
# ‚ïü‚îÄ55683178-82b0-11eb-21a0-118a2c5910ed
# ‚ïü‚îÄb9d1fd58-8830-11eb-2958-d93a88a14079
# ‚ïü‚îÄ2af0f768-882f-11eb-11ca-39645ca19ff5
# ‚ïü‚îÄ1db37098-8a8b-11eb-0e02-993f6e332c7b
# ‚ïü‚îÄ46ed3088-882f-11eb-1e36-b1766d7cf523
# ‚ïü‚îÄadc5fd18-8d7e-11eb-2367-1906b97d68fc
# ‚ïü‚îÄ1849dc46-8d7e-11eb-2e96-ebbd410da0b9
# ‚ïü‚îÄd066ae2e-8d7e-11eb-3c95-6b5ac74cd2af
# ‚ïü‚îÄf91cb75c-8d7e-11eb-23d4-83799d5f050f
# ‚ïü‚îÄf988df5e-8f36-11eb-2d8e-954e6e7986e6
# ‚ïü‚îÄ0632c6a2-8f37-11eb-09b7-237be141ef01
# ‚ïü‚îÄ61eaa072-8f38-11eb-3f3e-8d5160d91d3a
# ‚ïü‚îÄ75da65ae-8f38-11eb-1743-35205166384d
# ‚ïü‚îÄ37d09eb8-8f3a-11eb-08eb-2713d85bd6f6
# ‚ïü‚îÄ4308cefc-8f3a-11eb-25e6-774eac28ce02
# ‚ïü‚îÄe0e73616-8f3c-11eb-01fd-530ab31b3270
# ‚ïü‚îÄ220f8b34-8f3d-11eb-1b49-6be6a6faa650
